{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Soumik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Soumik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "from math import sqrt\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[] #corpus\n",
    "tokens_in_doc=[]\n",
    "\n",
    "\n",
    "def __remove_punctuation(text):\n",
    "\n",
    "        message = []\n",
    "        for x in text:\n",
    "            if x in string.punctuation:\n",
    "                message.append(' ')\n",
    "            else:\n",
    "                message.append(x)\n",
    "                \n",
    "        message = ''.join(message)\n",
    "\n",
    "        return message\n",
    "\n",
    "def __remove_stopwords(text):\n",
    "\n",
    "    words= []\n",
    "\n",
    "    for x in text.split():\n",
    "\n",
    "        x=x.lower()\n",
    "\n",
    "        if x in stopwords.words('english'):\n",
    "            pass\n",
    "        \n",
    "        elif x.isnumeric():\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # stemmer = nltk.stem.PorterStemmer()\n",
    "            # x = stemmer.stem(x)\n",
    "            lemma = nltk.wordnet.WordNetLemmatizer()   #lemmatization\n",
    "            x=lemma.lemmatize(x)\n",
    "            words.append(x)\n",
    "\n",
    "    return words\n",
    "\n",
    "def token_words(text):\n",
    "\n",
    "    message = __remove_punctuation(text)\n",
    "    words = __remove_stopwords(message)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [C:\\Users\\Soumik\\.wdm\\drivers\\chromedriver\\win32\\101.0.4951.41\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Soumik\\AppData\\Local\\Temp\\ipykernel_9700\\3352578384.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "urls = []\n",
    "titles = []\n",
    "\n",
    "\n",
    "with open('lt_url.txt') as f:\n",
    "\n",
    "    urls = f.readlines()\n",
    "\n",
    "with open('lt_title.txt') as f:\n",
    "\n",
    "    titles = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1=urls[0]\n",
    "\n",
    "driver.get(url_1)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "ques = soup.find(\"div\", {\"class\": \"content__u3I1 question-content__JfgR\"})\n",
    "\n",
    "problem_text_html = ques.findChild(\"div\", recursive=False).encode('utf-8')\n",
    "\n",
    "problem_text_html = problem_text_html.decode('ascii', 'ignore')\n",
    "\n",
    "with open(\"problem_1.txt\", \"w+\") as f:\n",
    "    f.write(problem_text_html)\n",
    "\n",
    "first_problem_text = ques.get_text()\n",
    "\n",
    "first_problem_text = first_problem_text + \" \" + titles[0]\n",
    "\n",
    "doc.append([first_problem_text])\n",
    "\n",
    "first_problem_text = token_words(first_problem_text)\n",
    "\n",
    "tokens_in_doc.append(first_problem_text)\n",
    "\n",
    "total = set(first_problem_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for url in urls:\n",
    "\n",
    "    try:\n",
    "\n",
    "        if(cnt==0):\n",
    "            cnt+=1\n",
    "            continue\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        ques = soup.find(\"div\", {\"class\": \"content__u3I1 question-content__JfgR\"})\n",
    "\n",
    "        if(ques is None):\n",
    "            print(cnt, url)\n",
    "            titles.pop(cnt)\n",
    "            continue\n",
    "\n",
    "        problem_text_html = ques.findChild(\"div\", recursive=False).encode('utf-8')\n",
    "\n",
    "        problem_text_html = problem_text_html.decode('ascii', 'ignore')\n",
    "\n",
    "        with open(\"problem_\"+str(cnt+1)+\".txt\", \"w+\") as f:\n",
    "            f.write(problem_text_html)\n",
    "\n",
    "        problem_text = ques.get_text()\n",
    "\n",
    "        problem_text = problem_text + \" \" + titles[cnt]\n",
    "        \n",
    "        doc.append([problem_text])\n",
    "\n",
    "        problem_text = token_words(problem_text)\n",
    "\n",
    "        tokens_in_doc.append(problem_text)\n",
    "\n",
    "        total = total.union(set(problem_text))\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    except:\n",
    "        print(cnt)\n",
    "        titles.pop(cnt)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "\n",
    "for word in first_problem_text:\n",
    "    wordDictA[word]+=1\n",
    "\n",
    "df=pd.DataFrame([wordDictA])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt2=0\n",
    "\n",
    "for each_doc in tokens_in_doc:\n",
    "    \n",
    "    if(cnt2==0):\n",
    "        cnt2+=1\n",
    "        continue\n",
    "\n",
    "    wordDictB = dict.fromkeys(total, 0)\n",
    "    for word in each_doc:\n",
    "        wordDictB[word]+=1\n",
    "    \n",
    "    df = df.append([wordDictB], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "\n",
    "tf = pd.DataFrame([computeTF(df.iloc[0], tokens_in_doc[0])])\n",
    "\n",
    "for i in range(1,cnt):\n",
    "    tf_i = computeTF(df.iloc[i], tokens_in_doc[i])\n",
    "    tf=tf.append([tf_i], ignore_index=True)    #Converting to dataframe for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(data):\n",
    "\n",
    "    idfDict = {}\n",
    "    N = cnt\n",
    "    \n",
    "    idfDict = dict.fromkeys(data.keys(), 0)\n",
    "\n",
    "    for key in data.keys():\n",
    "\n",
    "        count= (df[key]>0).sum()\n",
    "\n",
    "        if(count!=0):\n",
    "            idfDict[key] = math.log10(N / (float(count)))   #if some  word occurs in all document, this will return 0, implying that it is least rare \n",
    "        else:\n",
    "            idfDict[key] = 0\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF(df)\n",
    "\n",
    "idf_df=pd.DataFrame([idfs])\n",
    "\n",
    "idf_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(tf.values*idf_df.values, columns=tf.columns, index=tf.index)\n",
    "\n",
    "keywords = tf_idf.columns.values.tolist()\n",
    "\n",
    "with open('keywords.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(' '.join(map(str, keywords)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude=[]\n",
    "\n",
    "def calculateMagnitude(v):\n",
    "    magnitude = 0\n",
    "    for j in range(0, len(tf_idf.columns)):\n",
    "        magnitude += pow((v[j]), 2.0)\n",
    "    return sqrt(magnitude * 1.0)\n",
    "\n",
    "ls = tf_idf.values.tolist()\n",
    "\n",
    "for i in range(0,cnt):\n",
    "    magnitude.append(calculateMagnitude(ls[i]))\n",
    "\n",
    "with open('magnitude.txt', 'w') as f:\n",
    "    f.write(' '.join(map(str, magnitude)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = tf_idf.to_numpy()\n",
    "\n",
    "for i in range(0,cnt):\n",
    "    for j in range(0,len(tf_idf.columns)):\n",
    "        if(arr[i][j]!=0):\n",
    "            with open('tf_idf.txt', 'a') as f:\n",
    "                f.write(str(i)+' '+str(j)+' '+str(arr[i][j])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df.to_csv(r'./idf.txt', header=None, index=None, sep=' ',mode='w')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29d7bbd736e5f89b70e81a0eb899818172636dd18be77104559aa18680732d47"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
